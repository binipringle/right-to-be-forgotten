---
layout: narrative
title: United States
author:
editor: Sabina Pringle
source:
---

In the United States, where the punitive carceral system is designed to oppress, third-party access to formerly incarcerated individuals' criminal legal records marks them for exclusion from opportunities to seek employment, housing, and participate in political processes. The principal justification for burdening individuals who have been convicted of criminal offenses with the ongoing stigma of having a criminal record is the argument that criminal records are public information and a search engine's right to freely publish public information about someone’s criminal record is protected by the First Amendment.

As stated earlier, there is considerable overlap between the right to privacy and the right to be forgotten. Here, they overlap because neither public nor private information, once published, can be removed from the internet. Even if criminal records become private records because they have been expunged or sealed, if they were published (even on a government website) they may still show up in search results on the web. If a resident of the United States, not having a right to be forgotten, claims that publication of information about expunged or sealed criminal records violates their right to privacy, a court will likely still not order a search engine to remove the information from the web because the implicit fundamental right to privacy in the Fourteenth Amendment and penumbras of other parts of the U.S. Constitution[^9] limits only government intrusion into individuals' right to privacy. Freedom from invasions of privacy by private actors - arguably including search engines - is not protected by the Constitution of the United States. There is an urgent need to amend privacy law in response to the wild proliferation of unregulated data merchants, and to enact law that protects the right to be forgotten so that we can do something about all the harmful data already out there on the internet.

Section 230!!!!!

---

To: 	Professor Joe Rosenberg
From: 	Sabina Pringle
Re: 	Assignment Memo for Week 6, 2.22.23 class (Reaction paper 3)
Date: 	February 25, 2023

In the Knight First Amendment Institute at Columbia University’s amicus brief in support of Google in Gonzalez v. Google, Counsel of Record Scott Wilkins argues that algorithms are necessary to make the internet useful, because without algorithms “many of the ‘vast democratic forums of the internet’ […] would be useless jumbles of information, like libraries of randomly shelved books, but on an almost unimaginable scale” (2). Point taken; a search engine needs algorithms to look through its index of web pages and return results most relevant to queries users input. We rely on search engines and they rely on algorithms, so Wilkins is right to stress the importance of algorithms for internet use. Recommendation algorithms, however, are another story. While some search engines, such as Google’s search engine, use recommendation algorithms to return search results based on a query, other search engines do not.  Thus, while algorithms are necessary for using the internet, we should make a clear distinction between algorithms and recommendation algorithms which are based on a user’s search history, viewing behavior and other activity on the internet.  
One of the central questions in Gonzalez v. Google is whether Google’s use of a recommendation algorithm makes Google the publisher of content selected by the recommendation algorithm, and, as publisher, not covered by the immunity Congress extended to platforms through section 230 of the Communications Decency Act.  The question Wilkins addresses in the Knight amicus brief is whether Google’s use of a recommendation algorithm materially contributes to alleged illegality.  In Gonzalez v. Google, the court found that Google’s use of recommendation algorithms did not materially contribute to incitement to terrorism leading to bombings by Daesh  in Paris in 2015. The court’s reasoning followed the Knight amicus brief, where Wilkins argued that Youtube “amplified” content and did not publish it, and was therefore “shielded by Section 230 […] because amplification, without more, does not amount to a ‘material contribution’ the the alleged illegality.” (4).
I have a big problem with the premise, afforded by Section 230, that internet service providers should either be liable or immune from liability for harms caused by content published through the services they provide. The combination of Section 230 and the First Amendment’s guarantee of freedom of speech are an explosive combination, and we urgently need legislation that regulates what internet service providers can and cannot do. Gonzalez v. Google reveals a failure in United States’ law, and Congress should look to Europe – and California, whose privacy law was largely modeled on Europe’s General Data Protection Regulation (GDPR) - to see how the regulation of harmful and illegal internet content can be better handled in the United States.
For example, the GDPR imposes strict rules on how companies can use and process personal data, including using algorithms to analyze user behavior and preferences. Under the GDPR, companies are required to obtain explicit consent from users before processing their personal data, and users have the right to access, modify, or delete their data (ChatGPT). The Digital Services Act (DSA) is a new set of regulations proposed by the European Union that would require large online platforms to share more data with regulators, including information about their algorithms (ChatGPT). Germany’s Network Enforcement Act (NetzDG) (2017) requires social media companies to remove illegal content within a certain timeframe, including content that promotes hate speech, terrorism, or other harmful behavior (ChatGPT).
The laws that govern and rules that regulate the internet in the United States, like so much else in the United States, indecently serve the market economy over human beings, especially over human beings who are marginalized, less digitally educated and more vulnerable than others. We urgently need to amend or rewrite the Communications Decency Act.


[^8]:(footnote saying what scarlet letter is)   

[^9]:See *Griswold v Connecticut* [1965] and other cases.

---

### Footnotes
